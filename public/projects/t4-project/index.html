<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Using human pose estimation to help Physical Therapists collect patient data | Ethan Trotter</title>
<meta name=keywords content><meta name=description content="Using computer vision and classic ML to collect movement data on patients"><meta name=author content="Ethan Trotter"><link rel=canonical href=http://localhost:1313/projects/t4-project/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.d72444526d7ecbdb0015438a7fa89054a658bf759d0542e2e5df81ce94b493ee.css integrity="sha256-1yREUm1+y9sAFUOKf6iQVKZYv3WdBULi5d+BzpS0k+4=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/projects/t4-project/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="http://localhost:1313/projects/t4-project/"><meta property="og:site_name" content="Ethan Trotter"><meta property="og:title" content="Using human pose estimation to help Physical Therapists collect patient data"><meta property="og:description" content="Using computer vision and classic ML to collect movement data on patients"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="projects"><meta property="article:published_time" content="2024-10-19T00:00:00+00:00"><meta property="article:modified_time" content="2024-10-19T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Using human pose estimation to help Physical Therapists collect patient data"><meta name=twitter:description content="Using computer vision and classic ML to collect movement data on patients"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Projects","item":"http://localhost:1313/projects/"},{"@type":"ListItem","position":2,"name":"Using human pose estimation to help Physical Therapists collect patient data","item":"http://localhost:1313/projects/t4-project/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Using human pose estimation to help Physical Therapists collect patient data","name":"Using human pose estimation to help Physical Therapists collect patient data","description":"Using computer vision and classic ML to collect movement data on patients","keywords":[],"articleBody":"1. Project Overview The T4 project was a state-of-the-art markerless pose estimation software used to evaluate gait - running and walking movement - in Physical Therapy (PT) patients. It was designed to address a set of problems encountered in physical therapy gait analysis:\nLack of quantitative assessment tools Large time demand for PT’s to manually review videos of patient gait patterns Lack of standardization and accuracy of gait evaluations We addressed these issues by collecting video data of patient gait in a specially designed room, creating custom datasets and labeling them, then using custom computer vision and machine learning models to measure patient movements.\n2. Solution Architecture 2.1. A Note on Data Security This project involved collecting patient data in a clinical environment. We ensured data integrity and patient safety by adhering to HIPAA Protected Health Information regulatory guidance. Therefore, data was handled entirely within the clinical environment, and utilized encrypted servers on premises and the cloud storage infrastructure (OneDrive) that was the property of the clinics that we worked with. Patients were educated on the data life cycle and thoroughly briefed on the implications before they signed waivers to allow the use of their data.\nFigure 1: Data Flow Overview Figure 2: Photos of Process 2.2. Data Collection Video data was collected from four angles (Figure 2, Section A and B) at specific speeds in 10-second blocks. Sony cameras were chosen for their ability to:\nBe mounted in fixed positions around the patient (front, sides, and rear) Record in sync, maintaining the same frame number across all cameras Capture high frame rates for more detailed data. This setup included a custom room with a modified treadmill (to reduce sagittal occlusion), the cameras, and a computer interface for clinicians. Before training the models used to identify joints and gait patterns, the T4 team and clinicians had to gather more than 400 video recordings of patients. The original dataset was composed of these samples and enabled the developments mentioned later.\nA semi-automated process was devised to enable quick and easy data collection during dataset development and after going live. The videos of a patient’s gait were recorded and uploaded into a patient’s folder. A Powershell script on the local encrypted server would watch for these events (dev/bin/WatchToFlip.ps1). The videos would be pre-processed by flipping the videos to the correct orientation, renaming them, and increasing contrast of the video.\n2.3. Data Processing These videos were then put into a processing directory where the product/stride/main.py script would be run. This script would use our pose estimation models to infer the position of joints, seen in Figure 2 Section B. This pixel data was stored in .h5 files. Basic trigonometry was used to calculate the angles between dots along specified axes (product/stride/angle_finder.py). After this process was completed, a .csv file containing all of the calculated angles was created, as well as a video with the joints and angles written on it.\nFinally, at the end of video and .csv creation, a machine learning model would be run on the .csv. It would take a row of angles, and predict the phase of gait that the individual was in during that row, see Figure 2 Section C. This was an entirely novel development, to our knowledge, and a specific request from the PT’s. Having the phase of gait labeled on the .csv and frame simplified the review process.\nFigure 3: Data Review and Reporting 2.4. Data Processing and Sharing PTs could review the .csv file by uploading it to our deployed Dash app, which enabled individualized patient data analysis. They could highlight the phase of gait, isolate the perspective of the movement plane (ex. Anterior Frontal), and look at the movement patterns. These reports also allowed the PT to input patient information and prescriptive exercise plans for the patient and email it to them from their own email addresses.\n2.5. Data Storage All raw and generated data was archived in patient folders on the local encrypted server and the HIPAA-compliant cloud service used by the clinics. This process was automated through cloud tools and Powershell scripts that continuously monitored relevant directories.\n3. Technical Development 3.1. Joint Identification with Neural Networks The Physical Therapists that conduct gait analysis go through extensive training to review patient movement patterns (usually directly watching them, but also with videos) and identify altered movement patterns. This process relies heavily on the relative relationship of the patient’s joints. To mirror this, our first step was to pinpoint the exact joint locations PTs focus on, down to specific bone protrusions. These reference points varied across the anterior, sagittal, and posterior planes, so we developed three separate models to identify the necessary joints for each perspective, which was documented for our labeling team.\nNext, we reviewed the literature and found a paper from Alexander Mathis, et al.. In this paper the team describes a process of labeling the “body parts” of animals in videos. Then, they fit a pre-trained Residual Neural Network (ResNet-50) to the data, a process called transfer learning. This process is documented in the GitHub repository DEEPLABCUT. The T4 team modified the DEEPLABCUT package to customize it to the practical requirements of this project.\nWe utilized the labeling and training tools provided by the DEEPLABCUT package to label a proprietary dataset that we curated. This dataset contained video data of more than 400 patients. We took care to ensure that the dataset was representative of the diversity (size, shape, skin color, etc.) of the population so that the models created from it would perform on every patient. We did this by estimating the normal distribution of our patient population for relevant variables, and sought to label patients on the outer edges of this distribution. In practice, this process proved successful.\nLabeling was completed by a team of Biomedical Engineers, with guidance and reviews from the PT’s. Additionally, as models were run with patient data, low performers were identified, and used to add to the dataset that we curated, creating a feedback loop that enhanced model performance over time.\nThis specialized dataset and fine-tuning of a ResNet-50 model allowed our models to outperform other markerless and marker-based pose estimation software, especially in handling occlusions, irregular gait, and patients at the extremes of the normal distribution.\n3.2. Phase of Gait Recognition Models Interviews with the Physical Therapists revealed that they were conducting manual frame-by-frame reviews of video data to identify specific phases of gait and review the patient’s posture in these phases. Automating this process became a key feature to reduce the time spent collecting quantitative data.\nBefore going further, here is a brief explanation of the phases of gait: The human gait cycle is divided into two main phases—swing (foot off the ground) and stance (foot on the ground)—which must occur for the gait cycle to be complete. Here are the sub-phases for the stance phase:\nInitial Strike: The first frame in which the foot is making contact with the ground following the swing phase Loading Response: The phase in between Initial Strike and Midstance. Midstance: The point of maximum triple flexion. Typically, when the midpoint of the foot is nearly in alignment with the hip “keypoint.” Terminal Stance: The propulsion phase in between Midstance and Terminal Stance. Toe Off: The last frame with the toe (and/or foot) still on the ground. Figure 4: Phase of Gait Labeling Process With these definitions, the labeling team went frame by frame and labeled a dataset for the phases of gait. The input data, in this case, was the angle information that was calculated by the joint identification models. These labels were randomly sampled and reviewed by a team of PT’s to ensure that the standards were upheld. A total of 4000 labeled rows were generated by the team.\nDue to the nature of phase labeling, this dataset was inherently unbalanced. Some phases represent transition states between instantaneous states. For example, the ‘Initial Strike’ phase was contained in one frame, while the ‘Loading Phase’ was usually spread between 10-15 frames. To address this, we used the data augmentation strategy Synthetic Minority Over-sampling Technique (SMOTE) to create synthetic examples of under-represented phases, and we undersampled the phases that were more represented. Despite initial concerns, models trained on SMOTE-augmented data outperformed those trained on imbalanced datasets. These dataset creation processes are documented in dev/pgr_ml/dataset_creator.py and systematic_test.py.\nA systematic testing suite compared the performance of six machine learning models: Linear (Multi-Class Logistic Regression), Non-linear (Decision Tree, SVM, K-Nearest Neighbor), and Ensemble (Random Forest, Extra Trees). We used this approach to experimentally explore what algorithm would perform best on this dataset. A k-fold cross validation test was used to provide a reliable estimate of model accuracy and reduce the risk of overfitting in a single train-test split. The results can be seen below in Figure 5.\nFigure 5: Boxplot of Accuracy and Standard Deviation of the K-fold Cross Validation Test Further hyperparameter tuning and increased size of the dataset created a model that was 98% accurate when trained with the Extra Trees algorithm. This approach can be reviewed in the dev/pgr_ml/prototyper.ipynb file.\n3.3. Data Analysis Features The details of how this data is generated have been covered above. The purpose of this section is to connect the output of the technical development to the original business problems encountered by the PT’s.\n3.3.1. Angles The Physical Therapists that we spoke to often complained that they lacked objective and quantitative tools to conduct gait analysis. In practice, the Standard of Care is often just the direct observation of a patient walking or running. The PT uses their intuition and experience to form a hypothesis of what the cause of altered movement patterns is. Then, they go through several iterative cycles to address the problem. More advanced clinics have begun using other tools. Some use video based analysis, and use software to draw on the screen to make measurements of the angles of a patient’s joints. But, this requires manual, frame-by-frame review. Often taking 4-7 days to complete.\nTo address these challenges, the T4 team sought to automate this process. Since the gold standard clinics already relied on a number of angles for diagnosis of gait deficiencies we started with these. Specifically, they requested a graph of the average angle of a specific joint over the percent of the gait cycle. In one case, a PT provided the following image as inspiration, see Figure 6. It is taken from an unknown source in the physical therapy literature.\nFigure 6: Example of Desired Output To generate a nearly identical graph we followed this process (See product/report_gen/gait_slicer.py):\nCalculate the angles using trigonometry and the packages numpy and math in python (Figure 7) Upload the .csv containing angles into the Dash Application (Mentioned in section 3.5) Import the data into a pandas dataframe Slice the dataframe into complete cycles by locating the “Initial Strike” phase Concatenate a new column to store the “Percent to Completion” data, and calculate that value for each cycle Reindex to percent complete, then graph the result Create standard deviation lines Figure 7: Calculation of Angles Example The resulting graphs were updated live as the PT’s used our dashboard application. A graph for every measured joint angle (32 joint angles in total) could be created and reviewed. Figure 8 offers an example of the resulting graph. By using the plotly and dash app packages, we were able to create interactive graphs that allowed the PT’s to highlight a point on the graph and see the specific X and Y values of that point. As well as zooming in and out, taking photos, and printing the output.\nFigure 8: Final Product 3.3.2. Phases of Gait One of the most time consuming aspects of reviewing gait footage is going through the videos of a patient’s gait frame-by-frame and annotating the frame to measure the joint angle. In the clinics that we worked with, they had a specialized technician that completed this step. But, the T4 team noticed that these phases could be strictly defined and standardized. Therefore, we generated the hypothesis that If the phases of gait are defined by objective standards, then we can use joint angle data and consistent labeling to automate this process.\nThrough subsequent experiments, alluded to in section 3.2. we generated a model that was able to do so with 98% accuracy. An example of this result can be seen in Figure 8, where the midpoint of the “Loading Phase” is highlighted. This is in accordance with the business requirement and example provided in Figure 6.\n3.4. App Deployment During the production phase of the T4 Movement project, a Dash App was hosted on a Heroku server. The app allowed PT’s from multiple clinics to simultaneously access the dashboard, securely upload the .csv file that contained gait data (angles and phases of gait for all 4 planes; Note: No PHI was stored in these files.), and conduct an in-depth review of the data using the analytical tools mentioned earlier.\n4. Results and Impact To assess the results, we’ll revisit the business problems from Section 1 and discuss the impact of using the T4 software.\nBusiness problems: Lack of quantitative assessment tools Large time demand for PT’s to manually review videos of patient gait patterns Lack of standardization and accuracy of gait evaluations Before T4, clinics used a DSLR on a tripod to record patient gait, often with inconsistent placement, leading to inaccurate angle measurements. The fastest clinic we encountered had a technician label a few joint angles in select gait phases based on PT requests. This process lacked standardization, varied across patients and visits, and took 4-7 days to complete. Additionally, joint measurement points and phase definitions were inconsistently communicated between PTs and technicians.\nWith the physical hardware setup in the clinic, we standardized data collection. With the datasets that we created by labeling joints in videos and phases of gait we created rigorous definitions that were adhered to and went through multiple review processes. These point address the third business problem mentioned - the need to improve standardization and accuracy during the evaluation process.\nThe software outlined here is purpose-built to solve the first business problem, lack of quantitative assessment tools. We ended up returning data on 32 joint angles simultaneously and synced to the frame on an average 10 strides per patient.\nThe entire data collection process went from 10 minutes using the old process to 2 minutes on average with the new process. The biggest difference came from data processing. The entire process for running model inferences, calculating angles, and generating aggregated data resources for the PT’s to review took 1.5 minutes. So, at the fastest, the old process could be completed in approximately 5760 minutes (4 days * 24 hours * 60 minutes) and we could generate a much larger volume of data in about 3.5 minutes. So in about 0.06% of the time. This addressed business problem number 2.\n5. Technologies Used Programming Languages and Data Analytics Libraries: Python - dash - imblearn - matplotlib - numpy - pandas - plotly - sklearn - statsmodels Powershell Script ","wordCount":"2514","inLanguage":"en","datePublished":"2024-10-19T00:00:00Z","dateModified":"2024-10-19T00:00:00Z","author":[{"@type":"Person","name":"Ethan Trotter"}],"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/projects/t4-project/"},"publisher":{"@type":"Organization","name":"Ethan Trotter","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Home (Alt + H)"><img src=http://localhost:1313/images/FulcrumIcon.png alt aria-label=logo height=55>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/projects/ title=Projects><span>Projects</span></a></li><li><a href=http://localhost:1313/blog/ title=Blog><span>Blog</span></a></li><li><a href=http://localhost:1313/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/projects/>Projects</a></div><h1 class="post-title entry-hint-parent">Using human pose estimation to help Physical Therapists collect patient data</h1><div class=post-meta><span title='2024-10-19 00:00:00 +0000 UTC'>October 19, 2024</span>&nbsp;·&nbsp;12 min&nbsp;·&nbsp;2514 words&nbsp;·&nbsp;Ethan Trotter&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/projects/t4-project/index.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#1-project-overview>1. Project Overview</a></li><li><a href=#2-solution-architecture>2. Solution Architecture</a><ul><li><a href=#21-a-note-on-data-security>2.1. A Note on Data Security</a></li><li><a href=#22-data-collection>2.2. Data Collection</a></li><li><a href=#23-data-processing>2.3. Data Processing</a></li><li><a href=#24-data-processing-and-sharing>2.4. Data Processing and Sharing</a></li><li><a href=#25-data-storage>2.5. Data Storage</a></li></ul></li><li><a href=#3-technical-development>3. Technical Development</a><ul><li><a href=#31-joint-identification-with-neural-networks>3.1. Joint Identification with Neural Networks</a></li><li><a href=#32-phase-of-gait-recognition-models>3.2. Phase of Gait Recognition Models</a></li><li><a href=#33-data-analysis-features>3.3. Data Analysis Features</a></li><li><a href=#34-app-deployment>3.4. App Deployment</a></li></ul></li><li><a href=#4-results-and-impact>4. Results and Impact</a><ul><li><a href=#business-problems>Business problems:</a></li></ul></li><li><a href=#5-technologies-used>5. Technologies Used</a><ul><li><a href=#programming-languages-and-data-analytics-libraries>Programming Languages and Data Analytics Libraries:</a></li></ul></li></ul></nav></div></details></div><div class=post-content><h2 id=1-project-overview>1. Project Overview<a hidden class=anchor aria-hidden=true href=#1-project-overview>#</a></h2><p>The T4 project was a state-of-the-art markerless pose estimation software used to evaluate gait - running and walking movement - in Physical Therapy (PT) patients. It was designed to address a set of problems encountered in physical therapy gait analysis:</p><ol><li>Lack of quantitative assessment tools</li><li>Large time demand for PT&rsquo;s to manually review videos of patient gait patterns</li><li>Lack of standardization and accuracy of gait evaluations</li></ol><p>We addressed these issues by collecting video data of patient gait in a specially designed room, creating custom datasets and labeling them, then using custom computer vision and machine learning models to measure patient movements.</p><h2 id=2-solution-architecture>2. Solution Architecture<a hidden class=anchor aria-hidden=true href=#2-solution-architecture>#</a></h2><h3 id=21-a-note-on-data-security>2.1. A Note on Data Security<a hidden class=anchor aria-hidden=true href=#21-a-note-on-data-security>#</a></h3><p>This project involved collecting patient data in a clinical environment. We ensured data integrity and patient safety by adhering to HIPAA Protected Health Information regulatory guidance. Therefore, data was handled entirely within the clinical environment, and utilized encrypted servers on premises and the cloud storage infrastructure (OneDrive) that was the property of the clinics that we worked with. Patients were educated on the data life cycle and thoroughly briefed on the implications before they signed waivers to allow the use of their data.</p><h4 id=figure-1-data-flow-overview>Figure 1: Data Flow Overview<a hidden class=anchor aria-hidden=true href=#figure-1-data-flow-overview>#</a></h4><p><img alt="alt text" loading=lazy src=/projects/t4-project/images/DataFlowOverview.png></p><h4 id=figure-2-photos-of-process>Figure 2: Photos of Process<a hidden class=anchor aria-hidden=true href=#figure-2-photos-of-process>#</a></h4><p><img alt="alt text" loading=lazy src=/projects/t4-project/images/TechDescriptionCombinedGraphic.png></p><h3 id=22-data-collection>2.2. Data Collection<a hidden class=anchor aria-hidden=true href=#22-data-collection>#</a></h3><p>Video data was collected from four angles (Figure 2, Section A and B) at specific speeds in 10-second blocks. Sony cameras were chosen for their ability to:</p><ol><li>Be mounted in fixed positions around the patient (front, sides, and rear)</li><li>Record in sync, maintaining the same frame number across all cameras</li><li>Capture high frame rates for more detailed data.</li></ol><p>This setup included a custom room with a modified treadmill (to reduce sagittal occlusion), the cameras, and a computer interface for clinicians. Before training the models used to identify joints and gait patterns, the T4 team and clinicians had to <strong>gather more than 400 video recordings of patients. The original dataset was composed of these samples</strong> and enabled the developments mentioned later.</p><p>A semi-automated process was devised to enable quick and easy data collection during dataset development and after going live. The videos of a patient&rsquo;s gait were recorded and uploaded into a patient&rsquo;s folder. A Powershell script on the local encrypted server would watch for these events (dev/bin/WatchToFlip.ps1). The videos would be pre-processed by flipping the videos to the correct orientation, renaming them, and increasing contrast of the video.</p><h3 id=23-data-processing>2.3. Data Processing<a hidden class=anchor aria-hidden=true href=#23-data-processing>#</a></h3><p>These videos were then put into a processing directory where the product/stride/main.py script would be run. This script would <strong>use our pose estimation models to infer the position of joints</strong>, seen in Figure 2 Section B. This pixel data was stored in .h5 files. Basic trigonometry was used to calculate the angles between dots along specified axes (product/stride/angle_finder.py). After this process was completed, <strong>a .csv file containing all of the calculated angles was created, as well as a video with the joints and angles written on it.</strong></p><p>Finally, at the end of video and .csv creation, a machine learning model would be run on the .csv. It would take a row of angles, and predict the phase of gait that the individual was in during that row, see Figure 2 Section C. This was an entirely <strong>novel development, to our knowledge, and a specific request from the PT&rsquo;s</strong>. Having the phase of gait labeled on the .csv and frame simplified the review process.</p><h4 id=figure-3-data-review-and-reporting>Figure 3: Data Review and Reporting<a hidden class=anchor aria-hidden=true href=#figure-3-data-review-and-reporting>#</a></h4><p><img alt="alt text" loading=lazy src=/projects/t4-project/images/VideoT4ReportPhoto.png></p><h3 id=24-data-processing-and-sharing>2.4. Data Processing and Sharing<a hidden class=anchor aria-hidden=true href=#24-data-processing-and-sharing>#</a></h3><p>PTs could review the .csv file by uploading it to our deployed Dash app, which enabled individualized patient data analysis. They could <strong>highlight the phase of gait, isolate the perspective of the movement plane (ex. Anterior Frontal), and look at the movement patterns</strong>. These reports also allowed the PT to input patient information and prescriptive exercise plans for the patient and email it to them from their own email addresses.</p><h3 id=25-data-storage>2.5. Data Storage<a hidden class=anchor aria-hidden=true href=#25-data-storage>#</a></h3><p>All raw and generated data was archived in patient folders on the local encrypted server and the HIPAA-compliant cloud service used by the clinics. This process was automated through cloud tools and Powershell scripts that continuously monitored relevant directories.</p><h2 id=3-technical-development>3. Technical Development<a hidden class=anchor aria-hidden=true href=#3-technical-development>#</a></h2><h3 id=31-joint-identification-with-neural-networks>3.1. Joint Identification with Neural Networks<a hidden class=anchor aria-hidden=true href=#31-joint-identification-with-neural-networks>#</a></h3><p>The Physical Therapists that conduct gait analysis go through extensive training to review patient movement patterns (usually directly watching them, but also with videos) and identify altered movement patterns. This process relies heavily on the relative relationship of the patient&rsquo;s joints. To mirror this, our first step was to pinpoint the exact joint locations PTs focus on, down to specific bone protrusions. These reference points varied across the anterior, sagittal, and posterior planes, so we developed three separate models to identify the necessary joints for each perspective, which was documented for our labeling team.</p><p>Next, we reviewed the literature and found a paper from <a href=https://www.nature.com/articles/s41593-018-0209-y>Alexander Mathis, et al.</a>. In this paper the team describes a process of labeling the &ldquo;body parts&rdquo; of animals in videos. Then, they fit a pre-trained Residual Neural Network (ResNet-50) to the data, a process called transfer learning. This process is documented in the GitHub repository <a href=https://github.com/DeepLabCut/DeepLabCut>DEEPLABCUT</a>. The T4 team modified the DEEPLABCUT package to customize it to the practical requirements of this project.</p><p>We utilized the labeling and training tools provided by the DEEPLABCUT package to label a proprietary dataset that we curated. This dataset contained video data of more than 400 patients. We took care to ensure that the dataset was representative of the diversity (size, shape, skin color, etc.) of the population so that the models created from it would perform on every patient. We did this by estimating the normal distribution of our patient population for relevant variables, and sought to label patients on the outer edges of this distribution. In practice, this process proved successful.</p><p>Labeling was completed by a team of Biomedical Engineers, with guidance and reviews from the PT&rsquo;s. Additionally, as models were run with patient data, low performers were identified, and used to add to the dataset that we curated, creating a feedback loop that enhanced model performance over time.</p><p><strong>This specialized dataset and fine-tuning of a ResNet-50 model allowed our models to outperform other markerless and marker-based pose estimation software, especially in handling occlusions, irregular gait, and patients at the extremes of the normal distribution.</strong></p><h3 id=32-phase-of-gait-recognition-models>3.2. Phase of Gait Recognition Models<a hidden class=anchor aria-hidden=true href=#32-phase-of-gait-recognition-models>#</a></h3><p>Interviews with the Physical Therapists revealed that they were conducting manual frame-by-frame reviews of video data to identify specific phases of gait and review the patient&rsquo;s posture in these phases. Automating this process became a key feature to reduce the time spent collecting quantitative data.</p><p>Before going further, here is a brief explanation of the phases of gait:
The human gait cycle is divided into two main phases—swing (foot off the ground) and stance (foot on the ground)—which must occur for the gait cycle to be complete.
Here are the sub-phases for the stance phase:</p><ul><li>Initial Strike: The first frame in which the foot is making contact with the ground following the swing phase</li><li>Loading Response: The phase in between Initial Strike and Midstance.</li><li>Midstance: The point of maximum triple flexion. Typically, when the midpoint of the foot is nearly in alignment with the hip “keypoint.”</li><li>Terminal Stance: The propulsion phase in between Midstance and Terminal Stance.</li><li>Toe Off: The last frame with the toe (and/or foot) still on the ground.</li></ul><h4 id=figure-4-phase-of-gait-labeling-process>Figure 4: Phase of Gait Labeling Process<a hidden class=anchor aria-hidden=true href=#figure-4-phase-of-gait-labeling-process>#</a></h4><p><img alt="alt text" loading=lazy src=/projects/t4-project/images/FrameLabelingProcess.png>
With these definitions, the labeling team went frame by frame and labeled a dataset for the phases of gait. The input data, in this case, was the angle information that was calculated by the joint identification models. These labels were randomly sampled and reviewed by a team of PT&rsquo;s to ensure that the standards were upheld. A total of 4000 labeled rows were generated by the team.</p><p>Due to the nature of phase labeling, this dataset was inherently unbalanced. Some phases represent transition states between instantaneous states. For example, the &lsquo;Initial Strike&rsquo; phase was contained in one frame, while the &lsquo;Loading Phase&rsquo; was usually spread between 10-15 frames. To address this, we used the data augmentation strategy Synthetic Minority Over-sampling Technique (SMOTE) to create synthetic examples of under-represented phases, and we undersampled the phases that were more represented. Despite initial concerns, models trained on SMOTE-augmented data outperformed those trained on imbalanced datasets. These dataset creation processes are documented in dev/pgr_ml/dataset_creator.py and systematic_test.py.</p><p>A systematic testing suite compared the performance of six machine learning models: Linear (Multi-Class Logistic Regression), Non-linear (Decision Tree, SVM, K-Nearest Neighbor), and Ensemble (Random Forest, Extra Trees). We used this approach to experimentally explore what algorithm would perform best on this dataset. A k-fold cross validation test was used to provide a reliable estimate of model accuracy and reduce the risk of overfitting in a single train-test split. The results can be seen below in Figure 5.</p><h4 id=figure-5-boxplot-of-accuracy-and-standard-deviation-of-the-k-fold-cross-validation-test>Figure 5: Boxplot of Accuracy and Standard Deviation of the K-fold Cross Validation Test<a hidden class=anchor aria-hidden=true href=#figure-5-boxplot-of-accuracy-and-standard-deviation-of-the-k-fold-cross-validation-test>#</a></h4><p><img alt="alt text" loading=lazy src=/projects/t4-project/images/BoxplotOfModels.png></p><p>Further hyperparameter tuning and increased size of the dataset created a model that was <strong>98% accurate</strong> when trained with the Extra Trees algorithm. This approach can be reviewed in the dev/pgr_ml/prototyper.ipynb file.</p><h3 id=33-data-analysis-features>3.3. Data Analysis Features<a hidden class=anchor aria-hidden=true href=#33-data-analysis-features>#</a></h3><p>The details of how this data is generated have been covered above. The purpose of this section is to connect the output of the technical development to the original business problems encountered by the PT&rsquo;s.</p><h4 id=331-angles>3.3.1. Angles<a hidden class=anchor aria-hidden=true href=#331-angles>#</a></h4><p>The Physical Therapists that we spoke to often complained that they lacked objective and quantitative tools to conduct gait analysis. In practice, the Standard of Care is often just the direct observation of a patient walking or running. The PT uses their intuition and experience to form a hypothesis of what the cause of altered movement patterns is. Then, they go through several iterative cycles to address the problem. More advanced clinics have begun using other tools. Some use video based analysis, and use software to draw on the screen to make measurements of the angles of a patient&rsquo;s joints. But, this requires manual, frame-by-frame review. Often taking 4-7 days to complete.</p><p>To address these challenges, the T4 team sought to automate this process. Since the gold standard clinics already relied on a number of angles for diagnosis of gait deficiencies we started with these. <strong>Specifically, they requested a graph of the average angle of a specific joint over the percent of the gait cycle.</strong> In one case, a PT provided the following image as inspiration, see Figure 6. It is taken from an unknown source in the physical therapy literature.</p><h4 id=figure-6-example-of-desired-output>Figure 6: Example of Desired Output<a hidden class=anchor aria-hidden=true href=#figure-6-example-of-desired-output>#</a></h4><p><img alt="alt text" loading=lazy src=/projects/t4-project/images/AngleGraphExample.png></p><p>To generate a nearly identical graph we followed this process (See product/report_gen/gait_slicer.py):</p><ol><li>Calculate the angles using trigonometry and the packages numpy and math in python (Figure 7)</li><li>Upload the .csv containing angles into the Dash Application (Mentioned in section 3.5)</li><li>Import the data into a pandas dataframe</li><li>Slice the dataframe into complete cycles by locating the &ldquo;Initial Strike&rdquo; phase</li><li>Concatenate a new column to store the &ldquo;Percent to Completion&rdquo; data, and calculate that value for each cycle</li><li>Reindex to percent complete, then graph the result</li><li>Create standard deviation lines</li></ol><h4 id=figure-7-calculation-of-angles-example>Figure 7: Calculation of Angles Example<a hidden class=anchor aria-hidden=true href=#figure-7-calculation-of-angles-example>#</a></h4><p><img alt="alt text" loading=lazy src=/projects/t4-project/images/CalculationExample.png></p><p>The resulting graphs were updated live as the PT&rsquo;s used our dashboard application. A graph for every measured joint angle (32 joint angles in total) could be created and reviewed. Figure 8 offers an example of the resulting graph. By using the plotly and dash app packages, we were able to create interactive graphs that allowed the PT&rsquo;s to highlight a point on the graph and see the specific X and Y values of that point. As well as zooming in and out, taking photos, and printing the output.</p><h4 id=figure-8-final-product>Figure 8: Final Product<a hidden class=anchor aria-hidden=true href=#figure-8-final-product>#</a></h4><p><img alt="alt text" loading=lazy src=/projects/t4-project/images/FinalGraph.png></p><h4 id=332-phases-of-gait>3.3.2. Phases of Gait<a hidden class=anchor aria-hidden=true href=#332-phases-of-gait>#</a></h4><p>One of the most time consuming aspects of reviewing gait footage is going through the videos of a patient&rsquo;s gait frame-by-frame and annotating the frame to measure the joint angle. In the clinics that we worked with, they had a specialized technician that completed this step. But, the T4 team noticed that these phases could be strictly defined and standardized. Therefore, we generated the hypothesis that <em>If the phases of gait are defined by objective standards, then we can use joint angle data and consistent labeling to automate this process.</em></p><p>Through subsequent experiments, alluded to in section 3.2. we generated a model that was able to do so with 98% accuracy. An example of this result can be seen in Figure 8, where the midpoint of the &ldquo;Loading Phase&rdquo; is highlighted. This is in accordance with the business requirement and example provided in Figure 6.</p><h3 id=34-app-deployment>3.4. App Deployment<a hidden class=anchor aria-hidden=true href=#34-app-deployment>#</a></h3><p>During the production phase of the T4 Movement project, a <a href=https://dash.plotly.com/>Dash App</a> was hosted on a Heroku server. The app allowed PT&rsquo;s from multiple clinics to simultaneously access the dashboard, securely upload the .csv file that contained gait data (angles and phases of gait for all 4 planes; Note: No PHI was stored in these files.), and conduct an in-depth review of the data using the analytical tools mentioned earlier.</p><h2 id=4-results-and-impact>4. Results and Impact<a hidden class=anchor aria-hidden=true href=#4-results-and-impact>#</a></h2><p>To assess the results, we&rsquo;ll revisit the business problems from Section 1 and discuss the impact of using the T4 software.</p><h3 id=business-problems>Business problems:<a hidden class=anchor aria-hidden=true href=#business-problems>#</a></h3><ol><li>Lack of quantitative assessment tools</li><li>Large time demand for PT&rsquo;s to manually review videos of patient gait patterns</li><li>Lack of standardization and accuracy of gait evaluations</li></ol><p>Before T4, clinics used a DSLR on a tripod to record patient gait, often with inconsistent placement, leading to inaccurate angle measurements. The fastest clinic we encountered had a technician label a few joint angles in select gait phases based on PT requests. This process lacked standardization, varied across patients and visits, and took 4-7 days to complete. Additionally, joint measurement points and phase definitions were inconsistently communicated between PTs and technicians.</p><p>With the physical hardware setup in the clinic, we standardized data collection. With the datasets that we created by labeling joints in videos and phases of gait we created rigorous definitions that were adhered to and went through multiple review processes. These point address the third business problem mentioned - the need to improve standardization and accuracy during the evaluation process.</p><p>The software outlined here is purpose-built to solve the first business problem, lack of quantitative assessment tools. We ended up returning data on 32 joint angles simultaneously and synced to the frame on an average 10 strides per patient.</p><p>The entire data collection process went from 10 minutes using the old process to 2 minutes on average with the new process. The biggest difference came from data processing. The entire process for running model inferences, calculating angles, and generating aggregated data resources for the PT&rsquo;s to review took 1.5 minutes. So, at the fastest, the old process could be completed in approximately 5760 minutes (4 days * 24 hours * 60 minutes) and we could generate a much larger volume of data in about 3.5 minutes. So in about 0.06% of the time. This addressed business problem number 2.</p><h2 id=5-technologies-used>5. Technologies Used<a hidden class=anchor aria-hidden=true href=#5-technologies-used>#</a></h2><h3 id=programming-languages-and-data-analytics-libraries>Programming Languages and Data Analytics Libraries:<a hidden class=anchor aria-hidden=true href=#programming-languages-and-data-analytics-libraries>#</a></h3><ul><li>Python
- dash
- imblearn
- matplotlib
- numpy
- pandas
- plotly
- sklearn
- statsmodels</li><li>Powershell Script</li></ul></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=http://localhost:1313/projects/data-driven-marketing/><span class=title>« Prev</span><br><span>Data Driven Marketing: Pattern Recognition and Optimizing Campaigns with SQL and Stats</span>
</a><a class=next href=http://localhost:1313/projects/tiktok-niche/><span class=title>Next »</span><br><span>Organic Marketing: How to enter a new tiktok niche</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Using human pose estimation to help Physical Therapists collect patient data on x" href="https://x.com/intent/tweet/?text=Using%20human%20pose%20estimation%20to%20help%20Physical%20Therapists%20collect%20patient%20data&amp;url=http%3a%2f%2flocalhost%3a1313%2fprojects%2ft4-project%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Using human pose estimation to help Physical Therapists collect patient data on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fprojects%2ft4-project%2f&amp;title=Using%20human%20pose%20estimation%20to%20help%20Physical%20Therapists%20collect%20patient%20data&amp;summary=Using%20human%20pose%20estimation%20to%20help%20Physical%20Therapists%20collect%20patient%20data&amp;source=http%3a%2f%2flocalhost%3a1313%2fprojects%2ft4-project%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Using human pose estimation to help Physical Therapists collect patient data on reddit" href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fprojects%2ft4-project%2f&title=Using%20human%20pose%20estimation%20to%20help%20Physical%20Therapists%20collect%20patient%20data"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Using human pose estimation to help Physical Therapists collect patient data on facebook" href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fprojects%2ft4-project%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Using human pose estimation to help Physical Therapists collect patient data on whatsapp" href="https://api.whatsapp.com/send?text=Using%20human%20pose%20estimation%20to%20help%20Physical%20Therapists%20collect%20patient%20data%20-%20http%3a%2f%2flocalhost%3a1313%2fprojects%2ft4-project%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Using human pose estimation to help Physical Therapists collect patient data on telegram" href="https://telegram.me/share/url?text=Using%20human%20pose%20estimation%20to%20help%20Physical%20Therapists%20collect%20patient%20data&amp;url=http%3a%2f%2flocalhost%3a1313%2fprojects%2ft4-project%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Using human pose estimation to help Physical Therapists collect patient data on ycombinator" href="https://news.ycombinator.com/submitlink?t=Using%20human%20pose%20estimation%20to%20help%20Physical%20Therapists%20collect%20patient%20data&u=http%3a%2f%2flocalhost%3a1313%2fprojects%2ft4-project%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=http://localhost:1313/>Ethan Trotter</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>